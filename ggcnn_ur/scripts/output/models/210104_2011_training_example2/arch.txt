----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 16, 300, 300]           1,952
              ReLU-2         [-1, 16, 300, 300]               0
            Conv2d-3         [-1, 16, 300, 300]           6,416
              ReLU-4         [-1, 16, 300, 300]               0
         MaxPool2d-5         [-1, 16, 150, 150]               0
            Conv2d-6         [-1, 16, 150, 150]           6,416
              ReLU-7         [-1, 16, 150, 150]               0
            Conv2d-8         [-1, 16, 150, 150]           6,416
              ReLU-9         [-1, 16, 150, 150]               0
        MaxPool2d-10           [-1, 16, 75, 75]               0
           Conv2d-11           [-1, 32, 75, 75]          12,832
             ReLU-12           [-1, 32, 75, 75]               0
           Conv2d-13           [-1, 32, 75, 75]          25,632
             ReLU-14           [-1, 32, 75, 75]               0
UpsamplingBilinear2d-15         [-1, 32, 150, 150]               0
           Conv2d-16         [-1, 16, 150, 150]           4,624
             ReLU-17         [-1, 16, 150, 150]               0
UpsamplingBilinear2d-18         [-1, 16, 300, 300]               0
           Conv2d-19         [-1, 16, 300, 300]           2,320
             ReLU-20         [-1, 16, 300, 300]               0
           Conv2d-21          [-1, 1, 300, 300]              17
           Conv2d-22          [-1, 1, 300, 300]              17
           Conv2d-23          [-1, 1, 300, 300]              17
           Conv2d-24          [-1, 1, 300, 300]              17
================================================================
Total params: 66,676
Trainable params: 66,676
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.34
Forward/backward pass size (MB): 110.55
Params size (MB): 0.25
Estimated Total Size (MB): 111.15
----------------------------------------------------------------
(ggcnn) hanwen@hanwen-Precision-3640-Tower:~/ggcnn/ggcnn_hw$ python train_ggcnn.py --description training_example2 --network ggcnn2 --dataset cornell --dataset-path ./mydataset/ --batches-per-epoch 800
INFO:root:Loading Cornell Dataset...
INFO:root:Done
INFO:root:Loading Raw Network...
INFO:root:Done
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 16, 300, 300]           1,952
              ReLU-2         [-1, 16, 300, 300]               0
            Conv2d-3         [-1, 16, 300, 300]           6,416
              ReLU-4         [-1, 16, 300, 300]               0
         MaxPool2d-5         [-1, 16, 150, 150]               0
            Conv2d-6         [-1, 16, 150, 150]           6,416
              ReLU-7         [-1, 16, 150, 150]               0
            Conv2d-8         [-1, 16, 150, 150]           6,416
              ReLU-9         [-1, 16, 150, 150]               0
        MaxPool2d-10           [-1, 16, 75, 75]               0
           Conv2d-11           [-1, 32, 75, 75]          12,832
             ReLU-12           [-1, 32, 75, 75]               0
           Conv2d-13           [-1, 32, 75, 75]          25,632
             ReLU-14           [-1, 32, 75, 75]               0
UpsamplingBilinear2d-15         [-1, 32, 150, 150]               0
           Conv2d-16         [-1, 16, 150, 150]           4,624
             ReLU-17         [-1, 16, 150, 150]               0
UpsamplingBilinear2d-18         [-1, 16, 300, 300]               0
           Conv2d-19         [-1, 16, 300, 300]           2,320
             ReLU-20         [-1, 16, 300, 300]               0
           Conv2d-21          [-1, 1, 300, 300]              17
           Conv2d-22          [-1, 1, 300, 300]              17
           Conv2d-23          [-1, 1, 300, 300]              17
           Conv2d-24          [-1, 1, 300, 300]              17
================================================================
Total params: 66,676
Trainable params: 66,676
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.34
Forward/backward pass size (MB): 110.55
Params size (MB): 0.25
Estimated Total Size (MB): 111.15
----------------------------------------------------------------
INFO:root:Beginning Epoch 00
INFO:root:Epoch: 0, Batch: 100, Loss: 0.0485
INFO:root:Epoch: 0, Batch: 200, Loss: 0.0375
INFO:root:Epoch: 0, Batch: 300, Loss: 0.0330
INFO:root:Epoch: 0, Batch: 400, Loss: 0.0328
INFO:root:Epoch: 0, Batch: 500, Loss: 0.0409
INFO:root:Epoch: 0, Batch: 600, Loss: 0.0350
INFO:root:Epoch: 0, Batch: 700, Loss: 0.0443
INFO:root:Validating...
INFO:root:0/249 = 0.000000
INFO:root:Beginning Epoch 01
INFO:root:Epoch: 1, Batch: 100, Loss: 0.0494
INFO:root:Epoch: 1, Batch: 200, Loss: 0.0456
INFO:root:Epoch: 1, Batch: 300, Loss: 0.0516
INFO:root:Epoch: 1, Batch: 400, Loss: 0.0433
INFO:root:Epoch: 1, Batch: 500, Loss: 0.0323
INFO:root:Epoch: 1, Batch: 600, Loss: 0.0362
INFO:root:Epoch: 1, Batch: 700, Loss: 0.0323
INFO:root:Validating...
INFO:root:0/249 = 0.000000
INFO:root:Beginning Epoch 02
INFO:root:Epoch: 2, Batch: 100, Loss: 0.0453
INFO:root:Epoch: 2, Batch: 200, Loss: 0.0355
INFO:root:Epoch: 2, Batch: 300, Loss: 0.0423
INFO:root:Epoch: 2, Batch: 400, Loss: 0.0393
INFO:root:Epoch: 2, Batch: 500, Loss: 0.0366
INFO:root:Epoch: 2, Batch: 600, Loss: 0.0403
INFO:root:Epoch: 2, Batch: 700, Loss: 0.0286
INFO:root:Validating...
INFO:root:0/249 = 0.000000
INFO:root:Beginning Epoch 03
INFO:root:Epoch: 3, Batch: 100, Loss: 0.0469
INFO:root:Epoch: 3, Batch: 200, Loss: 0.0514
INFO:root:Epoch: 3, Batch: 300, Loss: 0.0330
INFO:root:Epoch: 3, Batch: 400, Loss: 0.0247
INFO:root:Epoch: 3, Batch: 500, Loss: 0.0401
INFO:root:Epoch: 3, Batch: 600, Loss: 0.0451
INFO:root:Epoch: 3, Batch: 700, Loss: 0.0405
INFO:root:Validating...
INFO:root:64/249 = 0.257028
INFO:root:Beginning Epoch 04
INFO:root:Epoch: 4, Batch: 100, Loss: 0.0424
INFO:root:Epoch: 4, Batch: 200, Loss: 0.0465
INFO:root:Epoch: 4, Batch: 300, Loss: 0.0490
INFO:root:Epoch: 4, Batch: 400, Loss: 0.0389
INFO:root:Epoch: 4, Batch: 500, Loss: 0.0396
INFO:root:Epoch: 4, Batch: 600, Loss: 0.0323
INFO:root:Epoch: 4, Batch: 700, Loss: 0.0313
INFO:root:Validating...
INFO:root:66/249 = 0.265060
INFO:root:Beginning Epoch 05
INFO:root:Epoch: 5, Batch: 100, Loss: 0.0361
INFO:root:Epoch: 5, Batch: 200, Loss: 0.0268
INFO:root:Epoch: 5, Batch: 300, Loss: 0.0318
INFO:root:Epoch: 5, Batch: 400, Loss: 0.0305
INFO:root:Epoch: 5, Batch: 500, Loss: 0.0311
INFO:root:Epoch: 5, Batch: 600, Loss: 0.0517
INFO:root:Epoch: 5, Batch: 700, Loss: 0.0290
INFO:root:Validating...
INFO:root:115/249 = 0.461847
INFO:root:Beginning Epoch 06
INFO:root:Epoch: 6, Batch: 100, Loss: 0.0301
INFO:root:Epoch: 6, Batch: 200, Loss: 0.0385
INFO:root:Epoch: 6, Batch: 300, Loss: 0.0238
INFO:root:Epoch: 6, Batch: 400, Loss: 0.0238
INFO:root:Epoch: 6, Batch: 500, Loss: 0.0339
INFO:root:Epoch: 6, Batch: 600, Loss: 0.0324
INFO:root:Epoch: 6, Batch: 700, Loss: 0.0259
INFO:root:Validating...
INFO:root:140/249 = 0.562249
INFO:root:Beginning Epoch 07
INFO:root:Epoch: 7, Batch: 100, Loss: 0.0223
INFO:root:Epoch: 7, Batch: 200, Loss: 0.0302
INFO:root:Epoch: 7, Batch: 300, Loss: 0.0380
INFO:root:Epoch: 7, Batch: 400, Loss: 0.0241
INFO:root:Epoch: 7, Batch: 500, Loss: 0.0240
INFO:root:Epoch: 7, Batch: 600, Loss: 0.0439
INFO:root:Epoch: 7, Batch: 700, Loss: 0.0211
INFO:root:Validating...
INFO:root:132/249 = 0.530120
INFO:root:Beginning Epoch 08
INFO:root:Epoch: 8, Batch: 100, Loss: 0.0313
INFO:root:Epoch: 8, Batch: 200, Loss: 0.0197
INFO:root:Epoch: 8, Batch: 300, Loss: 0.0350
INFO:root:Epoch: 8, Batch: 400, Loss: 0.0169
INFO:root:Epoch: 8, Batch: 500, Loss: 0.0266
INFO:root:Epoch: 8, Batch: 600, Loss: 0.0262
INFO:root:Epoch: 8, Batch: 700, Loss: 0.0205
INFO:root:Validating...
INFO:root:176/249 = 0.706827
INFO:root:Beginning Epoch 09
INFO:root:Epoch: 9, Batch: 100, Loss: 0.0262
INFO:root:Epoch: 9, Batch: 200, Loss: 0.0285
INFO:root:Epoch: 9, Batch: 300, Loss: 0.0335
INFO:root:Epoch: 9, Batch: 400, Loss: 0.0159
INFO:root:Epoch: 9, Batch: 500, Loss: 0.0317
INFO:root:Epoch: 9, Batch: 600, Loss: 0.0298
INFO:root:Epoch: 9, Batch: 700, Loss: 0.0219
INFO:root:Validating...
INFO:root:124/249 = 0.497992
INFO:root:Beginning Epoch 10
INFO:root:Epoch: 10, Batch: 100, Loss: 0.0248
INFO:root:Epoch: 10, Batch: 200, Loss: 0.0209
INFO:root:Epoch: 10, Batch: 300, Loss: 0.0228
INFO:root:Epoch: 10, Batch: 400, Loss: 0.0265
INFO:root:Epoch: 10, Batch: 500, Loss: 0.0198
INFO:root:Epoch: 10, Batch: 600, Loss: 0.0235
INFO:root:Epoch: 10, Batch: 700, Loss: 0.0151
INFO:root:Validating...
INFO:root:172/249 = 0.690763
INFO:root:Beginning Epoch 11
INFO:root:Epoch: 11, Batch: 100, Loss: 0.0138
INFO:root:Epoch: 11, Batch: 200, Loss: 0.0258
INFO:root:Epoch: 11, Batch: 300, Loss: 0.0180
INFO:root:Epoch: 11, Batch: 400, Loss: 0.0171
INFO:root:Epoch: 11, Batch: 500, Loss: 0.0188
INFO:root:Epoch: 11, Batch: 600, Loss: 0.0216
INFO:root:Epoch: 11, Batch: 700, Loss: 0.0153
INFO:root:Validating...
INFO:root:187/249 = 0.751004
INFO:root:Beginning Epoch 12
INFO:root:Epoch: 12, Batch: 100, Loss: 0.0174
INFO:root:Epoch: 12, Batch: 200, Loss: 0.0187
INFO:root:Epoch: 12, Batch: 300, Loss: 0.0294
INFO:root:Epoch: 12, Batch: 400, Loss: 0.0362
INFO:root:Epoch: 12, Batch: 500, Loss: 0.0208
INFO:root:Epoch: 12, Batch: 600, Loss: 0.0295
INFO:root:Epoch: 12, Batch: 700, Loss: 0.0163
INFO:root:Validating...
INFO:root:159/249 = 0.638554
INFO:root:Beginning Epoch 13
INFO:root:Epoch: 13, Batch: 100, Loss: 0.0157
INFO:root:Epoch: 13, Batch: 200, Loss: 0.0153
INFO:root:Epoch: 13, Batch: 300, Loss: 0.0191
INFO:root:Epoch: 13, Batch: 400, Loss: 0.0142
INFO:root:Epoch: 13, Batch: 500, Loss: 0.0198
INFO:root:Epoch: 13, Batch: 600, Loss: 0.0197
INFO:root:Epoch: 13, Batch: 700, Loss: 0.0204
INFO:root:Validating...
INFO:root:195/249 = 0.783133
INFO:root:Beginning Epoch 14
INFO:root:Epoch: 14, Batch: 100, Loss: 0.0157
INFO:root:Epoch: 14, Batch: 200, Loss: 0.0128
INFO:root:Epoch: 14, Batch: 300, Loss: 0.0162
INFO:root:Epoch: 14, Batch: 400, Loss: 0.0116
INFO:root:Epoch: 14, Batch: 500, Loss: 0.0156
INFO:root:Epoch: 14, Batch: 600, Loss: 0.0197
INFO:root:Epoch: 14, Batch: 700, Loss: 0.0141
INFO:root:Validating...
INFO:root:195/249 = 0.783133
INFO:root:Beginning Epoch 15
INFO:root:Epoch: 15, Batch: 100, Loss: 0.0126
INFO:root:Epoch: 15, Batch: 200, Loss: 0.0136
INFO:root:Epoch: 15, Batch: 300, Loss: 0.0209
INFO:root:Epoch: 15, Batch: 400, Loss: 0.0115
INFO:root:Epoch: 15, Batch: 500, Loss: 0.0146
INFO:root:Epoch: 15, Batch: 600, Loss: 0.0171
INFO:root:Epoch: 15, Batch: 700, Loss: 0.0105
INFO:root:Validating...
INFO:root:148/249 = 0.594378
INFO:root:Beginning Epoch 16
INFO:root:Epoch: 16, Batch: 100, Loss: 0.0133
INFO:root:Epoch: 16, Batch: 200, Loss: 0.0111
INFO:root:Epoch: 16, Batch: 300, Loss: 0.0150
INFO:root:Epoch: 16, Batch: 400, Loss: 0.0192
INFO:root:Epoch: 16, Batch: 500, Loss: 0.0183
INFO:root:Epoch: 16, Batch: 600, Loss: 0.0234
INFO:root:Epoch: 16, Batch: 700, Loss: 0.0125
INFO:root:Validating...
INFO:root:191/249 = 0.767068
INFO:root:Beginning Epoch 17
INFO:root:Epoch: 17, Batch: 100, Loss: 0.0135
INFO:root:Epoch: 17, Batch: 200, Loss: 0.0148
INFO:root:Epoch: 17, Batch: 300, Loss: 0.0114
INFO:root:Epoch: 17, Batch: 400, Loss: 0.0112
INFO:root:Epoch: 17, Batch: 500, Loss: 0.0177
INFO:root:Epoch: 17, Batch: 600, Loss: 0.0141
INFO:root:Epoch: 17, Batch: 700, Loss: 0.0176
INFO:root:Validating...
INFO:root:160/249 = 0.642570
INFO:root:Beginning Epoch 18
INFO:root:Epoch: 18, Batch: 100, Loss: 0.0152
INFO:root:Epoch: 18, Batch: 200, Loss: 0.0166
INFO:root:Epoch: 18, Batch: 300, Loss: 0.0123
INFO:root:Epoch: 18, Batch: 400, Loss: 0.0136
INFO:root:Epoch: 18, Batch: 500, Loss: 0.0117
INFO:root:Epoch: 18, Batch: 600, Loss: 0.0119
INFO:root:Epoch: 18, Batch: 700, Loss: 0.0101
INFO:root:Validating...
INFO:root:144/249 = 0.578313
INFO:root:Beginning Epoch 19
INFO:root:Epoch: 19, Batch: 100, Loss: 0.0118
INFO:root:Epoch: 19, Batch: 200, Loss: 0.0233
INFO:root:Epoch: 19, Batch: 300, Loss: 0.0175
INFO:root:Epoch: 19, Batch: 400, Loss: 0.0193
INFO:root:Epoch: 19, Batch: 500, Loss: 0.0113
INFO:root:Epoch: 19, Batch: 600, Loss: 0.0145
INFO:root:Epoch: 19, Batch: 700, Loss: 0.0114
INFO:root:Validating...
INFO:root:175/249 = 0.702811
INFO:root:Beginning Epoch 20
INFO:root:Epoch: 20, Batch: 100, Loss: 0.0138
INFO:root:Epoch: 20, Batch: 200, Loss: 0.0111
INFO:root:Epoch: 20, Batch: 300, Loss: 0.0142
INFO:root:Epoch: 20, Batch: 400, Loss: 0.0109
INFO:root:Epoch: 20, Batch: 500, Loss: 0.0081
INFO:root:Epoch: 20, Batch: 600, Loss: 0.0129
INFO:root:Epoch: 20, Batch: 700, Loss: 0.0095
INFO:root:Validating...
INFO:root:151/249 = 0.606426
INFO:root:Beginning Epoch 21
INFO:root:Epoch: 21, Batch: 100, Loss: 0.0171
INFO:root:Epoch: 21, Batch: 200, Loss: 0.0142
INFO:root:Epoch: 21, Batch: 300, Loss: 0.0119
INFO:root:Epoch: 21, Batch: 400, Loss: 0.0150
INFO:root:Epoch: 21, Batch: 500, Loss: 0.0126
INFO:root:Epoch: 21, Batch: 600, Loss: 0.0088
INFO:root:Epoch: 21, Batch: 700, Loss: 0.0158
INFO:root:Validating...
INFO:root:159/249 = 0.638554
INFO:root:Beginning Epoch 22

